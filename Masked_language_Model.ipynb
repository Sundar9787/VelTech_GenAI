{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "hiEPEDhlnYNw"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from scipy.special import softmax\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the pre-trained model to use: BERT-base-cased\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "# Instantiate the tokenizer and model for the specified pre-trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "055Aa5sJpdEg",
        "outputId": "21354678-f334-48cc-e306-ab26b68ef5eb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the mask token from the tokenizer\n",
        "mask = tokenizer.mask_token\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLpr5PvVplEs",
        "outputId": "6fe2b454-653f-4d29-f92a-ff44d4797a85"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sentence with a mask token to be filled in by the model\n",
        "sentence = f\"Sakthi is a {mask} boy.\"\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJaKqmt_pp2q",
        "outputId": "da02c454-542f-46bf-d0da-5a1c4697f1d9"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sa', '##kt', '##hi', 'is', 'a', '[MASK]', 'boy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the sentence using the tokenizer and return the input tensors\n",
        "encoded_inputs = tokenizer(sentence, return_tensors='pt')\n",
        "print(encoded_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1yEd0Vbpvmo",
        "outputId": "532db6a0-1909-416f-81f3-1fb87dd38fce"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101, 17784, 21270,  3031,  1110,   170,   103,  2298,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model's output for the input tensors\n",
        "outputs = model(**encoded_inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inh6740vrAT5",
        "outputId": "0d316c4b-bb68-430d-d95a-65cfc2b481f2"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MaskedLMOutput(loss=None, logits=tensor([[[ -7.4375,  -7.4023,  -7.4773,  ...,  -6.2403,  -5.8518,  -6.3604],\n",
            "         [ -8.0492,  -8.2752,  -8.0847,  ...,  -6.7218,  -6.4944,  -6.9420],\n",
            "         [ -2.6042,  -3.2494,  -3.2575,  ...,  -2.1986,  -2.3829,  -3.2883],\n",
            "         ...,\n",
            "         [ -8.1409,  -8.2819,  -8.2319,  ...,  -7.1964,  -5.6401,  -7.9355],\n",
            "         [-11.5900, -11.7678, -12.3174,  ...,  -8.5492,  -9.8181, -11.9257],\n",
            "         [-10.6881, -11.0931, -10.8000,  ...,  -8.1838,  -7.5953,  -9.8777]]],\n",
            "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detach the logits from the model's output and convert them to numpy arrays\n",
        "logits = outputs.logits.detach().numpy()[0]\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNR0U_NGrEws",
        "outputId": "5f84837b-5f0a-4977-bd99-fd3a4395b84f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 28996)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIQSyajRtkik",
        "outputId": "71dee6bc-1c6c-43fc-b171-49d33c9a38e4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the logits for the mask token\n",
        "mask_logits = logits[tokens.index(mask) + 1]\n",
        "print(mask_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLgtjefPtoF4",
        "outputId": "d47e3e00-0629-4bf2-bebe-02ab6c42b30f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-4.629001  -5.2313967 -4.738371  ... -5.890812  -4.155588  -5.180725 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confidence scores for each possible token using softmax\n",
        "confidence_scores= softmax(mask_logits)\n",
        "print(confidence_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgBPNldQugpI",
        "outputId": "739b866a-b64e-4820-ee31-25d2c7f6de54"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.0484790e-08 2.7640334e-08 4.5254488e-08 ... 1.4294299e-08 8.1051489e-08\n",
            " 2.9077006e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confidence_scores.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co5Gif5SumTV",
        "outputId": "6f80f3a4-8d51-4dbf-8e6f-2a5b43643b7a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99999994"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 5 predicted tokens and their confidence scores\n",
        "for i in np.argsort(confidence_scores)[::-1][:10]:\n",
        "    pred_token = tokenizer.decode(i)\n",
        "    score = confidence_scores[i]\n",
        "\n",
        "    # Print the predicted sentence with the mask token replaced by the predicted token, and the confidence score\n",
        "    print(sentence.replace(mask, pred_token), score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2U-0nv1uqQa",
        "outputId": "a295b11e-675d-41d9-aa64-72de56427f2f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sakthi is a young boy. 0.12496645\n",
            "Sakthi is a village boy. 0.0932745\n",
            "Sakthi is a little boy. 0.06266502\n",
            "Sakthi is a teenage boy. 0.04408817\n",
            "Sakthi is a beautiful boy. 0.03869896\n",
            "Sakthi is a small boy. 0.033369146\n",
            "Sakthi is a baby boy. 0.0219025\n",
            "Sakthi is a rich boy. 0.016079716\n",
            "Sakthi is a shy boy. 0.014640891\n",
            "Sakthi is a simple boy. 0.010145077\n"
          ]
        }
      ]
    }
  ]
}